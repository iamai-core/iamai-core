## Housekeeping
- [ ] Pull Collin branch interface.cpp code into main
- [ ] Figure out how to include the common lib from llama.cpp
- [ ] Add dev setup documentation

## Architecture
- [ ] Seperate tokenizing out from generate() function (so tokens go in and out of generate)
- [ ] Calculate the entropy of each prediction
- [ ] Get multiple input and output projection layers working

## Future
- [ ] Make "Server" branch for remote token generate server for Android app
- [ ] How to make async chat work using CLI? (maybe 2 terminals)
- [ ] Get tts working (llama.cpp/tools/tts)
- [ ] Test out current training support
